---
layout: post
title:  "2D CNN LSTM for SER implementation"
date:   2021-05-07 17:51:35 +0900
categories: MachineLearning
---

&nbsp;&nbsp;I implementated 2D CNN LSTM for SER in Zhao et al. (2019) [1]. You can see full code in [here][2Dgithub].

[2Dgithub]: https://github.com/buaaaaang/20210507-2D-CNN-LSTM

&nbsp;&nbsp;I also implented 1D CNN LSTM [here][1Dgithub], but paper mentioned that 2D CNN LSTM network 'outperforms' the traditional approaches, so I mainly focused on 2D model. 1D model use raw audio file as data, but 2D model preprocess data with log-mel spectogram. This is why it use 2D convolution. LFLB (local feature learning block) that consists convolutionar layer, batch normalization layer, exponential linear unit layer, and max pooling layer, was used to learn local features. LSTM was used to learn global feature. Dense layer was at the end of the model.
My goal was to apply the model to Korean speech dataset. The data was from [here][link]. It is 감정 분류를 위한 대화 음성 데이터셋 of AIHUB.

[1Dgithub]: https://github.com/buaaaaang/20210504-1D-CNN-LSTM
[link]: https://aihub.or.kr/keti_data_board/language_intelligence

&nbsp;&nbsp;Preprocessing was done by librosa. Result of preprocessing is data with 251 frames and 128 mel frequency bins. following picture is sample result of preprocessing

<p align="center">
  <img width="460" src="/assets/2021-05-07-2D-CNN-LSTM/log_mel_sample.png">
</p>

&nbsp;&nbsp;Model was built with Keras, and hyperparamter tuning was done by KerasTuner. Here is network's training accuracy and validation accuracy on Berlin EmoDB set in speaker-dependent experiments.

<p align="center">
  <img width="460" src="/assets/2021-05-07-2D-CNN-LSTM/acc-epoch_original_B.png">
</p>

Validation accuracy was 0.73, which is quite similar to the result of [1]. (which was 0.76)

&nbsp;&nbsp;Speaker dependent experiment on Korean speech dataset was done. Here is graph of training accuracy and validation accuracy.

<p align="center">
  <img width="460" src="/assets/2021-05-07-2D-CNN-LSTM/acc-epoch_original_K.png">
</p>

Validation accuracy was 0.47, which was not as good as expected

&nbsp;&nbsp;At this point, I pointed out suspicious part of this paper. The input data, which has size (128&times;251&times;1), pass through 4 LFLB layers, and become (1&times;1&times;128). Then, to connect this with LSTM layer, we should put reshape layer to change shape into (1&times;128). Here, time information is not contained anymore, so recurrent neural network is called only once. This means there is no meaning of using LSTM layer. To check this, I replaced LSTM layer with two dense layers. Here is the result when changed model was applied to Berlin EmoDB dataset.

<p align="center">
  <img width="460" src="/assets/2021-05-07-2D-CNN-LSTM/acc-epoch_dense_B.png">
</p>

validation accuracy wa 0.76, which is higher than the original model, showing that LSTM layer in the original model is not meaningful.

&nbsp;&nbsp;To use LSTM in meaningful way, we have to conserve time information of the data. I tried to change pooling layer to have smaller pooling size on frame-axis. When this modified model was applied to Berilin EmoDB dataset, I got better result(!) then the paper. The validation accuracy increased to 0.80.

<p align="center">
  <img width="460" src="/assets/2021-05-07-2D-CNN-LSTM/acc-epoch_modified_B.png">
</p>

&nbsp;&nbsp;I applied modified model to Korean speech dataset.

<p align="center">
  <img width="460" src="/assets/2021-05-07-2D-CNN-LSTM/acc-epoch_modified_K.png">
</p>

Validation accuracy was 0.49, which is slightly better, but still not that useful.

speaker-independent
about possibilities of model
how to deal with korean speech data



<br/>
<br/>

[1] Zhao, J., Mao, X., & Chen, L. (2019). Speech emotion recognition using deep 1D & 2D CNN LSTM networks. Biomedical Signal Processing and Control, 47, 312-323.

