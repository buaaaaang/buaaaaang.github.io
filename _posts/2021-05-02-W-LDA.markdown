---
layout: post
title:  "W-LDA implementation"
date:   2021-05-02 17:51:35 +0900
categories: MachineLearning
---

&nbsp;&nbsp;This work was done on fall of 2020, in group study supervised by Prof.Chun of KAIST. You can see our PPT named "LDA and Wasserstein LDA" in official [lab site][lab-site].

[lab-site]: https://chunhyonho.github.io/Group-study/Journal_club/
&nbsp;&nbsp;We implementated W-LDA proposed in Nan et al. 2019 [1]. LDA (Latent Dirichlet Allocation) is the most popular probabilistic topic model, but it is hard to use VAE directly to LDA because LDA use Direchlet distribution, which does not belong to location-scale family. This paper use WAE model proposed in [2] to make W-LDA model, that use Dirichlet prior on the latent document topic vectors.

&nbsp;&nbsp;WAE in [2] is a model that can use any distribution as prior. Since the distribution may not belong to location scale family, we cannot use reparametrization trick, so WAE use deterministic encoder. Instead, in 4.3 of [1], they added big noise to document-topic vectors during training. 

&nbsp;&nbsp;We used tensorflow and keras for implementation. following is the code for model:

{% highlight ruby %}
class WAE(keras.Model):
    def __init__(self, encoder, decoder, noise=0.2, lamb=1.,alpha=[0.1]*5, **kwargs):
        super(WAE, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
        self.noise = noise
        self.lamb = lamb
        self.alpha=alpha

    def reconstructionLoss(self,w,w_hat):
        #getting reconstruction loss (cross-entropy loss)
        batch_size = w.shape[0]
        eps = 10**-12
        w_hat = tf.clip_by_value(w_hat,eps,1)
        log_w_hat = tf.math.log(w_hat)
        c = tf.multiply(w,log_w_hat)
        c = tf.reduce_sum(c)
        return -1. * c/batch_size
    
    def diffusion_kernel(self,tensor):
        return tf.math.exp(-1*tf.math.square(tf.math.acos(tensor)))
    
    def mmdLoss(self,false_theta,true_theta):
        #getting MMD loss
        batch_size = false_theta.shape[0]
        assert batch_size==100, "wrong batch size"
        eps = 10**-12
        f = tf.math.sqrt(tf.clip_by_value(false_theta,eps,1))
        t = tf.math.sqrt(tf.clip_by_value(true_theta,eps,1))
        ft = tf.clip_by_value(tf.matmul(f,t,transpose_b=True),0,1-eps)
        ff = tf.clip_by_value(tf.matmul(f,f,transpose_b=True),0,1-eps)
        tt = tf.clip_by_value(tf.matmul(t,t,transpose_b=True),0,1-eps)
        ft = self.diffusion_kernel(ft)
        ff = self.diffusion_kernel(ff)
        tt = self.diffusion_kernel(tt)
        ft = 2*tf.reduce_sum(ft)/(batch_size**2)
        ff = tf.reduce_sum(ff)/(batch_size*(batch_size-1))
        tt = tf.reduce_sum(tt)/(batch_size*(batch_size-1))
        return tt + ff - ft

    def train_step(self, w):
        with tf.GradientTape() as tape:
            theta = self.encoder(w)
            theta_noise = tf.constant(np.random.dirichlet(self.alpha,size=w.shape[0]))
            theta_plus = (1-self.noise)*theta + self.noise*theta_noise
            w_hat = self.decoder(theta_plus)
            true_theta = tf.constant(np.random.dirichlet(self.alpha,size=w.shape[0]))
            loss_reconstruction = self.reconstructionLoss(w,w_hat) 
            loss_mmd = self.mmdLoss(theta,true_theta)
            loss = loss_reconstruction + loss_mmd*self.lamb
        grads = tape.gradient(loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
        return {
            "loss": loss,
            "reconstruction loss": loss_reconstruction,
            "mmd loss": loss_mmd,
        }
{% endhighlight %}

&nbsp;&nbsp;the model was initialized and trained as following:

{% highlight ruby %}
    #n_elements represents structure of encoder 
    encoder_input = keras.Input(shape=(n_elements[0]))
    encoder_layer = keras.layers.Dense(n_elements[1],activation="relu")(encoder_input)
    for i in range(2,len(n_elements)-1):  
        encoder_layer = keras.layers.Dense(n_elements[i],activation="relu")(encoder_layer)
    encoder_output = keras.layers.Dense(n_elements[-1],activation="softmax")(encoder_layer)
    encoder = keras.Model(encoder_input,encoder_output,name="encoder")
            
    decoder_input = keras.Input(shape=(n_elements[-1]))
    decoder_output = keras.layers.Dense(n_elements[0],activation="softmax")(decoder_input)
    decoder = keras.Model(decoder_input,decoder_output,name="decoder")

    with open('corpus.pickle','rb') as f:
        corpus = pickle.load(f)

    wae = WAE(encoder,decoder)
    wae.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001,beta_1=0.99,beta_2=0.999))
{% endhighlight %}

&nbsp;&nbsp;the model was applied to following two generated corpus. the graph represents the frequency of the words (1-100) in each topic. 

![Corpus](/assets/2021-05-02-W-LDA/corpus.png){:class="img-responsive"}

&nbsp;&nbsp;As in paper, we took top-ten words of the model (those with high beta values). We can observe that the model works well for left corpus, which is easier. (order doesn't matter for right one because probability for top 10 words are all the same) For right corpus, the model does not work well.

![Top10Words](/assets/2021-05-02-W-LDA/top10words.png){:class="img-responsive"}

&nbsp;&nbsp;We noticed that layer of decoder should not use softmax function as activation function, because probability of words in certain documents is determined by linearly adding up beta value for each topics. So we modified W-LDA model to have linear layer for decoder.

{% highlight ruby %}
decoder_output = keras.layers.Dense(n_elements[0],activation="relu",
    kernel_constraint=keras.constraints.NonNeg(),
    kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.001, maxval=0.1))(decoder_input)
{% endhighlight %}

&nbsp;&nbsp;the modified works better on generated corpus.

<p align="center">
  <img width="460" src="/assets/2021-05-02-W-LDA/top10words_modified.png">
</p>

&nbsp;&nbsp;Another good thing about using modified model is that we can interpret weight of decoder as beta values. We can infer how corpus was made by plotting beta values as following:

<p align="center">
  <img width="460" src="/assets/2021-05-02-W-LDA/corpusInfer.png">
</p>

&nbsp;&nbsp;As a result, we successfully implemented W-LDA proposed in [1], and we could modify the model to have better performance on generated corpus.

<br/>
<br/>

[1] Nan, F., Ding, R., Nallapati, R., & Xiang, B. (2019). Topic Modeling with Wasserstein Autoencoders.\
[2] Tolstikhin, I., Bousquet, O., Gelly, S., & Schoelkopf, B. (2017). Wasserstein Auto-Encoders.